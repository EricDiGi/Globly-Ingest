{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1dnBVyMxPsJ80d8mtT1i0ysSq9IL-Hh1O",
      "authorship_tag": "ABX9TyOiBdcGqv7pCwS8z/hJduxX",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/EricDiGi/Globly-Ingest/blob/main/Invoice_Upload_V2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "cellView": "form",
        "id": "ZklbavXd8OOn"
      },
      "outputs": [],
      "source": [
        "# @title Import Control for Difficult Libraries\n",
        "import os, sys\n",
        "\n",
        "# module name & command line entry if it should not be found\n",
        "imports = [\n",
        "    [\"snowflake.connector\", \"pip install snowflake-connector-python[pandas]==2.7.9\"],\n",
        "    [\"snowflake.sqlalchemy\", \"pip install --upgrade snowflake-sqlalchemy\"]\n",
        "]\n",
        "\n",
        "def safe_import(module, command):\n",
        "  try:\n",
        "    __import__(module)\n",
        "  except ModuleNotFoundError:\n",
        "    os.system(command)\n",
        "    __import__(module)\n",
        "\n",
        "for mod, cmd in imports:\n",
        "  safe_import(mod, cmd)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title All the Imports\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random as r\n",
        "import re\n",
        "import glob\n",
        "\n",
        "import yaml\n",
        "from yaml import SafeLoader\n",
        "\n",
        "from getpass import getpass\n",
        "\n",
        "import multiprocessing as mp\n",
        "from multiprocessing import Process, Lock, Pool\n",
        "\n",
        "\n",
        "from snowflake.connector.pandas_tools import pd_writer\n",
        "import snowflake.connector\n",
        "from snowflake.sqlalchemy import URL\n",
        "from sqlalchemy import create_engine\n",
        "from sqlalchemy.orm import scoped_session, sessionmaker\n",
        "\n",
        "import traceback\n",
        "import warnings\n",
        "warnings.filterwarnings(\"error\")\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "import logging\n",
        "logging.getLogger('snowflake.connector.cursor').setLevel(logging.WARNING) "
      ],
      "metadata": {
        "cellView": "form",
        "id": "GEO7emaLgH6c"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Verbose Print & Logging System\n",
        "# @markdown Log file contains info on which files were uploaded and which weren't\n",
        "\"\"\"\n",
        "  Valid-bool  Note  Path  Sheet\n",
        "\"\"\"\n",
        "def vprint(input_string, end=\"\\n\", flush=True, log=False):\n",
        "  def __log():\n",
        "    try:\n",
        "      with open(log_file_, 'a') as log_stream:\n",
        "        log_stream.write(input_string+end)\n",
        "        log_stream.flush()\n",
        "    except:\n",
        "      raise Exception(\"No log stream available\")\n",
        "\n",
        "  def __sub():\n",
        "    if 'verbose_' not in globals():\n",
        "      print(input_string, end=end)\n",
        "      if flush:\n",
        "        sys.stdout.flush()\n",
        "    else:\n",
        "      if verbose_:\n",
        "        print(input_string, end=end)\n",
        "        if flush:\n",
        "          sys.stdout.flush()\n",
        "\n",
        "  if 'print_lock' in globals():\n",
        "    print_lock.acquire()\n",
        "    try:\n",
        "      __sub()\n",
        "    finally:\n",
        "      print_lock.release()\n",
        "  else:\n",
        "    __sub()\n",
        "  \n",
        "  if log and ('fsys_lock' in globals()) and (log_file_ in globals()):\n",
        "    fsys_lock.acquire()\n",
        "    try:\n",
        "      __log()\n",
        "    finally:\n",
        "      fsys_lock.release()\n",
        "  elif log and (log_file_ in globals()):\n",
        "    __log()"
      ],
      "metadata": {
        "id": "Qp5HdbrwqKCZ"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Database Login\n",
        "\n",
        "#-----------------------------------------------------\n",
        "#               OVERHEAD Variables\n",
        "#-----------------------------------------------------\n",
        "\n",
        "os.environ['SNOW_USER'] = input(\"Username: \")\n",
        "os.environ['SNOW_PASSWORD'] = getpass(\"Password: \")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "cellView": "form",
        "id": "4xmmu2c4j76_",
        "outputId": "ff3aa440-88cf-4396-de5b-a44b4f8608f8"
      },
      "execution_count": 5,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Username: eric_digioacchino\n",
            "Password: ··········\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Armory:\n",
        "  def __init__(self):\n",
        "    self.candidate_files = None\n",
        "  #-----------------------------------------------------\n",
        "  #               CONFIG Helpful Variables\n",
        "  #-----------------------------------------------------\n",
        "\n",
        "    # Config YAML\n",
        "    stream = open(\"drive/MyDrive/Invoice Ingestion Suite/config.yml\",\"r\")\n",
        "    self.config_ = yaml.load(stream, SafeLoader)\n",
        "    stream.close()\n",
        "    \n",
        "    # Customers\n",
        "    self.customer_regex = list()\n",
        "    with open(self.config_[\"logging\"][\"parent_dir\"]+self.config_['tool_files']['customer_regex'],\"r\") as f:\n",
        "      lines = f.readlines()\n",
        "      for line in lines:\n",
        "        self.customer_regex.append( [s.rstrip() for s in line.split(\"\\t\")] ) \n",
        "\n",
        "    # Column Names \n",
        "    self.column_name_regex = list()\n",
        "    with open(self.config_[\"logging\"][\"parent_dir\"]+self.config_['tool_files']['column_solve'],\"r\") as f:\n",
        "      lines = f.readlines()\n",
        "      for line in lines:\n",
        "        self.column_name_regex.append( [s.rstrip() for s in line.split(\"\\t\")] )\n",
        "\n",
        "\n",
        "    # GLOB Filename Sequences\n",
        "    self.filename_structures = list()\n",
        "    with open(self.config_[\"logging\"][\"parent_dir\"]+self.config_['tool_files']['filename_structs'],\"r\") as f:\n",
        "      self.filename_structures = f.readlines()\n",
        "      self.filename_structures = [line.rstrip() for line in self.filename_structures]\n",
        "\n",
        "    # Database Connection (Snowflake)\n",
        "    self.conn = snowflake.connector.connect(\n",
        "                  user = os.environ['SNOW_USER']\n",
        "                  , password = os.environ['SNOW_PASSWORD']\n",
        "                  , account = self.config_['snowflake']['account']\n",
        "                  , warehouse = self.config_['snowflake']['warehouse']\n",
        "                  , role = self.config_['snowflake']['role']\n",
        "                  , database = self.config_['snowflake']['database']\n",
        "                  , schema = self.config_['snowflake']['schema']\n",
        "                  )\n",
        "    self.engine = create_engine((self.config_[\"snowflake\"][\"_PATH\"][0] + self.config_['snowflake']['account'] + self.config_[\"snowflake\"][\"_PATH\"][1]), creator=lambda: self.conn)\n",
        "    #self.sql_session = sessionmaker(bind=self.engine)\n",
        "  #-----------------------------------------------------\n",
        "  #               Helpful Loaders and Processors\n",
        "  #-----------------------------------------------------\n",
        "\n",
        "  # Find all Candidate Files and load into list\n",
        "  def load(self):\n",
        "    self.candidate_files = []\n",
        "    for struct in self.filename_structures:\n",
        "      self.candidate_files += list(glob.glob(struct, recursive=True))\n",
        "\n",
        "  # Using file path, find the name of the customer\n",
        "  def get_customer(self,file_path):\n",
        "    found = None\n",
        "    for regex, customer in self.customer_regex:\n",
        "      rex = re.compile(regex)\n",
        "      res = rex.search(file_path)\n",
        "      if res != None:\n",
        "        return customer\n",
        "    return None\n",
        "  \n",
        "  # Validate that every file path has a valid customer regex\n",
        "  def validate(self):\n",
        "    found_bad = False\n",
        "    if self.candidate_files is None:\n",
        "      raise Exception(\"No files loaded for validation\")\n",
        "    for f in self.candidate_files:\n",
        "      cust = self.get_customer(f.lower())\n",
        "      if cust == None:\n",
        "        found_bad = True\n",
        "        vprint(\"Unknown Customer:%s\\n\"%(f) , end=\"\",flush=True)\n",
        "\n",
        "    if not found_bad:\n",
        "      vprint(\"Rules Compiled for all Customers\\n\" , end=\"\",flush=True)\n",
        "      return True\n",
        "    else:\n",
        "      vprint(\"Missing Customer, need to debug\\n\" , end=\"\",flush=True)\n",
        "      return False\n",
        "\n",
        "  #-----------------------------------------------------\n",
        "  #               SQL Stuff\n",
        "  #-----------------------------------------------------\n",
        "  def upload_to_snowflake(self, data_frame, engine, table_name, chunk=2000, truncate=True, create=False):\n",
        "    with self.engine.connect() as con:\n",
        "        if create:\n",
        "            data_frame.head(0).to_sql(name=table_name, con=self.conn, if_exists=\"replace\", index=False, chunksize=chunk)\n",
        "        if truncate:\n",
        "            con.execute(f\"truncate table {table_name}\")\n",
        "            con.close()\n",
        "        data_frame.to_sql(name=table_name, con=self.engine, if_exists='append', index=False, chunksize=chunk, method=pd_writer)\n",
        "    #self.engine.dispose()\n",
        "  \n",
        "  def sqlize(self, df, create=False):\n",
        "    df.columns = map(lambda x: str(x).upper(), df.columns)\n",
        "    self.upload_to_snowflake(df, self.engine, self.config_['terraform']['table_name'], truncate=False, create=create)\n",
        "  "
      ],
      "metadata": {
        "id": "EQaQWkhrB1-M"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Invoice:\n",
        "  def __init__(self, df, path, sheet, iter):\n",
        "    self.df = df\n",
        "    self.path = path\n",
        "    self.sheet = sheet\n",
        "    self.file_num = iter\n",
        "\n",
        "  def preprocess(self):\n",
        "    vprint(\"preprocessing - %d\\n\"%(self.file_num) , end=\"\",flush=True)\n",
        "    \n",
        "    cols = dict([(c,(''.join(ch for ch in c if ch.isalnum()).upper())) for c in self.df.columns])\n",
        "    self.df.rename(columns=cols, inplace=True)\n",
        "\n",
        "    # Do some cleaning here\n",
        "    customer_name = tools.get_customer(self.path.lower())\n",
        "    if customer_name == \"Customer Regex Failed\":\n",
        "      vprint(\"Customer name resolution failed:%s\\n\"%(self.path) , end=\"\", flush=True)\n",
        "    self.df['CUSTOMER'] = customer_name\n",
        "    self.df[\"file_path\"] = self.path\n",
        "\n",
        "    specialty_columns = ['RXPRICE', 'DISPENSINGFEE', 'UNITPRICE']\n",
        "    for col in specialty_columns:\n",
        "      try:\n",
        "        self.df[col+'TYPE'] = self.df[col].apply(type)\n",
        "        if len(self.df.index[self.df[col+'TYPE'] == str].tolist()) > 0:\n",
        "          self.df = self.df.truncate(\n",
        "              after=list([a-1 for a in self.df.index[(self.df[col+'TYPE'] == str)].tolist()])[0]\n",
        "              ,axis=0\n",
        "              )\n",
        "          # self.df.drop(labels=[1+a for a in self.df.index[(self.df[col+'TYPE'] == str)].tolist()], axis=0, inplace=True)\n",
        "          # self.df.drop(labels=self.df.index[(self.df[col+'TYPE'] == str)].tolist(), axis=0, inplace=True)\n",
        "        # drop calculations after meaningful data if known to exist\n",
        "      except Exception as e:\n",
        "        #print(e)\n",
        "        continue\n",
        "\n",
        "  def simplify(self):    \n",
        "    if type(self.df) == type(None):\n",
        "      vprint(\"0\\t\\\"Invoice Failed to Resolve\\\"\\t\\\"%s\\\"\\t\\\"%s\\\"\"%(self.path, self.sheet), log=True, flush=True)\n",
        "      return None\n",
        "\n",
        "    vprint(\"simplifying - %d\\n\"%(self.file_num) , end=\"\",flush=True)\n",
        "\n",
        "    _columns = self.df.columns.tolist()\n",
        "    simplified = pd.DataFrame()\n",
        "\n",
        "    self.df = self.df.convert_dtypes()\n",
        "    column_types = {\n",
        "        \"api_request_timestamp\": (lambda df, c: df[c].astype(str)),\n",
        "        \"zipcode\": (lambda df, c: df[c].astype(str)),\n",
        "        \"cash_insurance\": (lambda df, c: df[c].astype(str)),\n",
        "        \"dispensed_ndc\": (lambda df, c: df[c].astype(str)),\n",
        "        \"rx_ndc_concat\" : (lambda df, c: df[c].astype(str)),\n",
        "        \"rx_number\": (lambda df, c: df[c].astype(str)),\n",
        "        \"dispensing_fee\": (lambda df, c: pd.to_numeric(df[c], errors=\"coerce\")),\n",
        "        \"rx_price\": (lambda df, c: pd.to_numeric(df[c], errors=\"coerce\")),\n",
        "        \"unit_price\": (lambda df,c: pd.to_numeric(df[c], errors=\"coerce\")),\n",
        "        \"invoice_reason\": (lambda df, c: df[c].astype(str)),\n",
        "        \"shipping_cost\": (lambda df, c: pd.to_numeric(df[c],errors=\"coerce\"))\n",
        "    }\n",
        "\n",
        "    for col, reg in tools.column_name_regex:\n",
        "      rexcomp = re.compile(reg)\n",
        "      try:\n",
        "        simplified[col] = self.df[list(filter(rexcomp.match,_columns))].bfill(axis=1).iloc[:,0]\n",
        "        try:\n",
        "          simplified[col] = column_types[col](simplified,col)\n",
        "        except Exception as e1:\n",
        "          continue\n",
        "      except Exception as e2:\n",
        "        continue\n",
        "    \n",
        "    simplified['api_request_timestamp'] = pd.to_datetime(simplified['api_request_timestamp'], \n",
        "                                          #format='%Y-%m-%d %H:%M:%S', \n",
        "                                          utc=True,\n",
        "                                          errors='coerce')\n",
        "    \n",
        "    self.df = simplified\n",
        "    # vprint(self.df.dtypes)\n",
        "    # "
      ],
      "metadata": {
        "id": "7zIaNpmAAbzy"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CandidateFile:\n",
        "  def __init__(self,filepath, iter):\n",
        "    self.path = filepath\n",
        "    self.workbook = None\n",
        "    self.sheet = None\n",
        "    self.page = None\n",
        "    self.file_num = iter\n",
        "    \n",
        "    self.has_valid = False\n",
        "    self.is_bad = False\n",
        "    self.__regex__ = re.compile(\"\\s*api.*\")\n",
        "  \n",
        "  def load(self):\n",
        "    fsys_lock.acquire()\n",
        "    #vprint(\"Loading - %d\\n\"%(self.file_num) , end=\"\",flush=True)\n",
        "    try:\n",
        "      with pd.ExcelFile(self.path) as workbook:\n",
        "        self.has_valid = self.verify_sheets(workbook)\n",
        "      #self.workbook.close()\n",
        "    except Exception as e:\n",
        "      self.is_bad = True\n",
        "      #vprint(\"0\\t\\\"Workbook Failed to Load\\\"\\t\\\"%s\\\"\\t\\\"%s\\\"\"%(self.path, self.sheet), log=True, flush=True)\n",
        "    finally:\n",
        "      #vprint(\"File System Lock to release - %d\"%(self.file_num), flush=True)\n",
        "      fsys_lock.release()\n",
        "  \n",
        "  def verify_sheets(self, workbook):\n",
        "    for sheet in workbook.sheet_names:\n",
        "      try:\n",
        "          self.page = pd.read_excel(self.path, sheet_name=sheet)\n",
        "          trimmed = list(filter(self.__regex__.match, list(self.page.columns.str.lower())))\n",
        "          if len(trimmed) >= 2:\n",
        "            self.sheet = sheet\n",
        "            return True\n",
        "      except Exception as e:\n",
        "        continue\n",
        "    return False\n",
        "  \n",
        "  def transfer_knowledge(self):\n",
        "    vprint(\"Transfering knowledge - %d\\n\"%(self.file_num) , end=\"\",flush=True)\n",
        "    if not self.is_bad and self.has_valid:\n",
        "      if type(self.page) != type(None) and self.page.empty == False:\n",
        "        return Invoice(self.page, self.path, self.sheet, self.file_num)\n",
        "    return None\n",
        "\n"
      ],
      "metadata": {
        "id": "aei3e44R8xat"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DocumentProcess:\n",
        "  def __init__(self, file, n):\n",
        "    self.candy = CandidateFile(file, n)\n",
        "    self.file_num = n\n",
        "  \n",
        "  def _run(self):\n",
        "    #vprint(f'Process %s started working on task %d'%(mp.current_process().name,self.file_num), flush=True)\n",
        "    invoice = None\n",
        "    vprint(\"Loading - %s\"%(self.file_num), flush=True)\n",
        "    self.candy.load()\n",
        "    vprint(\"Loading Complete - %s\"%(self.file_num), flush=True)\n",
        "    if self.candy.is_bad:\n",
        "      vprint(\"Rotten Egg - %d\"%(self.file_num), flush=True)\n",
        "    if self.candy.has_valid:\n",
        "      try:\n",
        "        invoice = self.candy.transfer_knowledge()\n",
        "        if invoice != None:\n",
        "          invoice.preprocess()\n",
        "          invoice.simplify()\n",
        "          vprint(\"done prepping - %d\\n\"%(self.file_num) , end=\"\",flush=True)\n",
        "          \n",
        "          if type(invoice.df) != type(None) and not invoice.df.empty:\n",
        "            sql_lock.acquire()\n",
        "            try:\n",
        "              vprint(\"trying to sqlize - %d\\n\"%(self.file_num) , end=\"\",flush=True)\n",
        "              \n",
        "\n",
        "              tools.sqlize(invoice.df)\n",
        "\n",
        "              vprint(\"Shared %d to DB\\n\"%(self.file_num), end=\"\",flush=True)\n",
        "              \n",
        "            except Exception as e:\n",
        "              vprint(\"0\\t\\\"DB Share Failed\\\"\\t\\\"%s\\\"\\t\\\"%s\\\"\"%(self.candy.path, self.candy.sheet), log=True, flush=True)\n",
        "              vprint(\"%s\\nCould not send %d to Database because %s\\n %s\\n\"%(invoice.df.dtypes,self.file_num, e, traceback.format_exc()), end=\"\",flush=True)\n",
        "              \n",
        "            finally:\n",
        "              sql_lock.release()\n",
        "            vprint(\"File %d Complete\\n\"%(self.file_num) , end=\"\",flush=True)\n",
        "            vprint(\"1\\t\\\"Upload Complete\\\"\\t\\\"%s\\\"\\t\\\"%s\\\"\"%(self.candy.path, self.candy.sheet), log=True, flush=True)\n",
        "          else:\n",
        "            vprint(\"0\\t\\\"Invoice Empty\\\"\\t\\\"%s\\\"\\t\\\"%s\\\"\"%(self.candy.path, self.candy.sheet), log=True, flush=True)\n",
        "            vprint(\"Invoice is empty for some reason\\n\", end=\"\",flush=True)\n",
        "\n",
        "      except Exception as e:\n",
        "        vprint(\"Sheet %d could not be processed because %s \\n %s\\n\"%(self.file_num,e, traceback.format_exc()) , end=\"\",flush=True)\n",
        "        \n",
        "    else:\n",
        "      vprint(\"0\\t\\\"No Sheets Found\\\"\\t\\\"%s\\\"\"%(self.candy.path), log=True, flush=True)\n",
        "      vprint(\"No Sheets Here - %d\\n\"%(self.file_num) , end=\"\",flush=True)\n",
        "    #vprint(f'Process %s finished working on task %d'%(mp.current_process().name,self.file_num), flush=True)\n",
        "      \n",
        "\n",
        "    "
      ],
      "metadata": {
        "id": "-0IM7o-CR0gZ"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_active_globals(input_series):\n",
        "  intersection = [i for i in input_series if i in globals()]\n",
        "  for var in intersection:\n",
        "    del globals()[var]"
      ],
      "metadata": {
        "id": "y3fY2KYNtYyF"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ColdRefresh:\n",
        "  def __init__(self):\n",
        "    pass\n",
        "\n",
        "  def refresh(self, log_file):\n",
        "    print(\"ColdRefresh\")\n",
        "    global tools\n",
        "    tools = Armory()\n",
        "    print(\"Traversing File Structure\")\n",
        "    tools.load()\n",
        "    print(\"All Files Explored\")\n",
        "    if tools.validate():\n",
        "      vprint(\"Tool Box is ready\\n\" , end=\"\",flush=True)\n",
        "    else:\n",
        "      raise Exception(\"Run Backtrace - Tool Set failed to validate\")\n",
        "    \n",
        "    global log_file_\n",
        "    log_file_ = tools.config_[\"logging\"][\"parent_dir\"] + log_file\n",
        "    if not os.path.exists(log_file_):\n",
        "      with open(log_file_, 'x') as f:\n",
        "        pass\n",
        "\n",
        "class HotRefresh(ColdRefresh):\n",
        "  def refresh(self, log_file):\n",
        "    print(\"HotRefresh\")\n",
        "\n",
        "    global tools\n",
        "    tools = Armory()\n",
        "    print(\"Traversing File Structure\")\n",
        "    tools.load()\n",
        "\n",
        "    # Remove seen files\n",
        "    global log_file_\n",
        "    log_file_ = tools.config_[\"logging\"][\"parent_dir\"] + log_file\n",
        "    if os.path.exists(log_file_):\n",
        "      try:\n",
        "        log_data = pd.read_table(log_file_, header=None)\n",
        "        seen_files = log_data[2].tolist()\n",
        "        tools.candidate_files = list(filter(lambda i: i not in seen_files, tools.candidate_files))\n",
        "      except:\n",
        "        pass\n",
        "    else:\n",
        "      with open(log_file_, 'x') as f:\n",
        "        pass\n",
        "\n",
        "    print(\"All Files Explored\")\n",
        "    if tools.validate():\n",
        "      vprint(\"Tool Box is ready\\n\" , end=\"\",flush=True)\n",
        "    else:\n",
        "      raise Exception(\"Run Backtrace - Tool Set failed to validate\")\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "iwmFeo7YTQDs"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "global tools\n",
        "tools = None\n",
        "\n",
        "def f(obj):\n",
        "    obj._run()\n",
        "    return obj\n",
        "\n",
        "def RunIngestionPipe(verbose=True, log=False, log_file=\"log.txt\", random_batch_size=None, max_pooling=3, refresh_method='ColdRefesh', debug=False, debug_list=None):\n",
        "  global verbose_, log_file_\n",
        "  verbose_ = verbose\n",
        "\n",
        "  # Do Refresh Stuff Here\n",
        "  refresh_sys = globals()[refresh_method]().refresh(log_file=log_file)\n",
        "  \n",
        "  global fsys_lock, sql_lock, print_lock\n",
        "  fsys_lock = Lock()\n",
        "  sql_lock = Lock()\n",
        "  print_lock = Lock()\n",
        "  if debug:\n",
        "    candidate_series =  ([DocumentProcess(f, n) for n,f in enumerate(debug_list)])\n",
        "  else:\n",
        "    if random_batch_size is not None:\n",
        "      candidate_series = ([DocumentProcess(f, n) for n,f in enumerate(r.sample(tools.candidate_files,random_batch_size))])\n",
        "    else:\n",
        "      candidate_series = ([DocumentProcess(f, n) for n,f in enumerate(tools.candidate_files)])\n",
        "    \n",
        "  vprint(\"Candidates Identified\\n\" , end=\"\",flush=True)\n",
        "  vprint(\"Beginning Ingestion\\n\" , end=\"\",flush=True)\n",
        "  with Pool(max_pooling) as p:\n",
        "    p.map(f, candidate_series)"
      ],
      "metadata": {
        "id": "rS5CNCTMQjC0"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "problem_files = [\n",
        "    ]\n",
        "    \n",
        "RunIngestionPipe(\n",
        "    verbose=True\n",
        "    , log=True\n",
        "    , max_pooling=7\n",
        "    #, random_batch_size=250\n",
        "    , debug=True\n",
        "    , debug_list=problem_files\n",
        "    , refresh_method='ColdRefresh'\n",
        "  )"
      ],
      "metadata": {
        "id": "ozxmxQsGoc8N"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}