{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1dnBVyMxPsJ80d8mtT1i0ysSq9IL-Hh1O",
      "authorship_tag": "ABX9TyPQHhBYnyeEG+2G6ngl2+GW",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/EricDiGi/Globly-Ingest/blob/main/Invoice_Upload_V2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "cellView": "form",
        "id": "ZklbavXd8OOn"
      },
      "outputs": [],
      "source": [
        "# @title Import Control for Difficult Libraries\n",
        "import os, sys\n",
        "\n",
        "# module name & command line entry if it should not be found\n",
        "imports = [\n",
        "    [\"snowflake.connector\", \"pip install snowflake-connector-python[pandas]==2.7.9\"],\n",
        "    [\"snowflake.sqlalchemy\", \"pip install --upgrade snowflake-sqlalchemy\"]\n",
        "]\n",
        "\n",
        "def safe_import(module, command):\n",
        "  try:\n",
        "    __import__(module)\n",
        "  except ModuleNotFoundError:\n",
        "    os.system(command)\n",
        "    __import__(module)\n",
        "\n",
        "for mod, cmd in imports:\n",
        "  safe_import(mod, cmd)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Verbose Print & Logging System\n",
        "# @markdown Log file contains info on which files were uploaded and which weren't\n",
        "\"\"\"\n",
        "  Valid-bool  Note  Path  Sheet\n",
        "\"\"\"\n",
        "def vprint(input_string, end=\"\\n\", flush=True, log=False):\n",
        "  def __log():\n",
        "    try:\n",
        "      with open(log_file_, 'a') as log_stream:\n",
        "        log_stream.write(input_string+end)\n",
        "    except:\n",
        "      raise Exception(\"No log stream available\")\n",
        "\n",
        "  def __sub():\n",
        "    if 'verbose_' not in globals():\n",
        "      print(input_string, end=end)\n",
        "      if flush:\n",
        "        sys.stdout.flush()\n",
        "    else:\n",
        "      if verbose_:\n",
        "        print(input_string, end=end)\n",
        "        if flush:\n",
        "          sys.stdout.flush()\n",
        "\n",
        "  if 'print_lock' in globals():\n",
        "    print_lock.acquire()\n",
        "    try:\n",
        "      __sub()\n",
        "    finally:\n",
        "      print_lock.release()\n",
        "  else:\n",
        "    __sub()\n",
        "  \n",
        "  if log and ('fsys_lock' in globals()):\n",
        "    fsys_lock.acquire()\n",
        "    try:\n",
        "      __log()\n",
        "    finally:\n",
        "      fsys_lock.release()\n",
        "  elif log:\n",
        "    __log()"
      ],
      "metadata": {
        "id": "Qp5HdbrwqKCZ"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# global log_file_\n",
        "# log_file_ = tools.config_[\"logging\"][\"parent_dir\"]+\"log.txt\"\n",
        "# #del fsys_lock\n",
        "# vprint(\"This is a test\", log=True)"
      ],
      "metadata": {
        "id": "QCKZiJGxeUIt"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title All the Imports\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random as r\n",
        "import re\n",
        "import glob\n",
        "\n",
        "import yaml\n",
        "from yaml import SafeLoader\n",
        "\n",
        "from getpass import getpass\n",
        "\n",
        "from multiprocessing import Process, Lock, Pool\n",
        "\n",
        "from snowflake.connector.pandas_tools import pd_writer\n",
        "import snowflake.connector\n",
        "from snowflake.sqlalchemy import URL\n",
        "from sqlalchemy import create_engine\n",
        "from sqlalchemy.orm import scoped_session, sessionmaker\n",
        "\n",
        "import traceback\n",
        "import warnings\n",
        "warnings.filterwarnings(\"error\")\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "import logging\n",
        "logging.getLogger('snowflake.connector.cursor').setLevel(logging.WARNING) "
      ],
      "metadata": {
        "cellView": "form",
        "id": "GEO7emaLgH6c"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Database Login\n",
        "\n",
        "#-----------------------------------------------------\n",
        "#               OVERHEAD Variables\n",
        "#-----------------------------------------------------\n",
        "\n",
        "os.environ['SNOW_USER'] = input(\"Username: \")\n",
        "os.environ['SNOW_PASSWORD'] = getpass(\"Password: \")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "cellView": "form",
        "id": "4xmmu2c4j76_",
        "outputId": "44db1f50-ee85-44a2-98a8-756a1159dade"
      },
      "execution_count": 5,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Username: eric_digioacchino\n",
            "Password: ··········\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Armory:\n",
        "  def __init__(self):\n",
        "    self.candidate_files = None\n",
        "  #-----------------------------------------------------\n",
        "  #               CONFIG Helpful Variables\n",
        "  #-----------------------------------------------------\n",
        "\n",
        "    # Config YAML\n",
        "    stream = open(\"drive/MyDrive/Invoice Ingestion Suite/config.yml\",\"r\")\n",
        "    self.config_ = yaml.load(stream, SafeLoader)\n",
        "    stream.close()\n",
        "    # Customers\n",
        "    self.customer_regex = list()\n",
        "    with open(self.config_[\"logging\"][\"parent_dir\"]+self.config_['tool_files']['customer_regex'],\"r\") as f:\n",
        "      lines = f.readlines()\n",
        "      for line in lines:\n",
        "        self.customer_regex.append( [s.rstrip() for s in line.split(\"\\t\")] ) \n",
        "\n",
        "    # Column Names \n",
        "    self.column_name_regex = list()\n",
        "    with open(self.config_[\"logging\"][\"parent_dir\"]+self.config_['tool_files']['column_solve'],\"r\") as f:\n",
        "      lines = f.readlines()\n",
        "      for line in lines:\n",
        "        self.column_name_regex.append( [s.rstrip() for s in line.split(\"\\t\")] )\n",
        "\n",
        "\n",
        "    # GLOB Filename Sequences\n",
        "    self.filename_structures = list()\n",
        "    with open(self.config_[\"logging\"][\"parent_dir\"]+self.config_['tool_files']['filename_structs'],\"r\") as f:\n",
        "      self.filename_structures = f.readlines()\n",
        "      self.filename_structures = [line.rstrip() for line in self.filename_structures]\n",
        "\n",
        "    # Database Connection (Snowflake)\n",
        "    self.conn = snowflake.connector.connect(\n",
        "                  user = os.environ['SNOW_USER']\n",
        "                  , password = os.environ['SNOW_PASSWORD']\n",
        "                  , account = self.config_['snowflake']['account']\n",
        "                  , warehouse = self.config_['snowflake']['warehouse']\n",
        "                  , role = self.config_['snowflake']['role']\n",
        "                  , database = self.config_['snowflake']['database']\n",
        "                  , schema = self.config_['snowflake']['schema']\n",
        "                  )\n",
        "    self.engine = create_engine((self.config_[\"snowflake\"][\"_PATH\"][0] + self.config_['snowflake']['account'] + self.config_[\"snowflake\"][\"_PATH\"][1]), creator=lambda: self.conn)\n",
        "    #self.sql_session = sessionmaker(bind=self.engine)\n",
        "  #-----------------------------------------------------\n",
        "  #               Helpful Loaders and Processors\n",
        "  #-----------------------------------------------------\n",
        "\n",
        "  # Find all Candidate Files and load into list\n",
        "  def load(self):\n",
        "    self.candidate_files = []\n",
        "    for struct in self.filename_structures:\n",
        "      self.candidate_files += list(glob.glob(struct, recursive=True))\n",
        "\n",
        "  # Using file path, find the name of the customer\n",
        "  def get_customer(self,file_path):\n",
        "    found = None\n",
        "    for regex, customer in self.customer_regex:\n",
        "      rex = re.compile(regex)\n",
        "      res = rex.search(file_path)\n",
        "      if res != None:\n",
        "        return customer\n",
        "    return None\n",
        "  \n",
        "  # Validate that every file path has a valid customer regex\n",
        "  def validate(self):\n",
        "    found_bad = False\n",
        "    if self.candidate_files is None:\n",
        "      raise Exception(\"No files loaded for validation\")\n",
        "    for f in self.candidate_files:\n",
        "      cust = self.get_customer(f.lower())\n",
        "      if cust == None:\n",
        "        found_bad = True\n",
        "        vprint(\"Unknown Customer:%s\\n\"%(f) , end=\"\",flush=True)\n",
        "\n",
        "    if not found_bad:\n",
        "      vprint(\"Rules Compiled for all Customers\\n\" , end=\"\",flush=True)\n",
        "      return True\n",
        "    else:\n",
        "      vprint(\"Missing Customer, need to debug\\n\" , end=\"\",flush=True)\n",
        "      return False\n",
        "\n",
        "  #-----------------------------------------------------\n",
        "  #               SQL Stuff\n",
        "  #-----------------------------------------------------\n",
        "  def upload_to_snowflake(self, data_frame, engine, table_name, chunk=2000, truncate=True, create=False):\n",
        "    with self.engine.connect() as con:\n",
        "        if create:\n",
        "            data_frame.head(0).to_sql(name=table_name, con=self.conn, if_exists=\"replace\", index=False, chunksize=chunk)\n",
        "        if truncate:\n",
        "            con.execute(f\"truncate table {table_name}\")\n",
        "            con.close()\n",
        "        data_frame.to_sql(name=table_name, con=self.engine, if_exists='append', index=False, chunksize=chunk, method=pd_writer)\n",
        "    #self.engine.dispose()\n",
        "  \n",
        "  def sqlize(self, df, create=False):\n",
        "    df.columns = map(lambda x: str(x).upper(), df.columns)\n",
        "    self.upload_to_snowflake(df, self.engine, self.config_['terraform']['table_name'], truncate=False, create=create)\n",
        "  "
      ],
      "metadata": {
        "id": "EQaQWkhrB1-M"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Invoice:\n",
        "  def __init__(self, df, path, sheet, iter):\n",
        "    self.df = df\n",
        "    self.path = path\n",
        "    self.sheet = sheet\n",
        "    self.file_num = iter\n",
        "\n",
        "  def preprocess(self):\n",
        "    vprint(\"preprocessing - %d\\n\"%(self.file_num) , end=\"\",flush=True)\n",
        "    \n",
        "    cols = dict([(c,(''.join(ch for ch in c if ch.isalnum()).upper())) for c in self.df.columns])\n",
        "    self.df.rename(columns=cols, inplace=True)\n",
        "\n",
        "    # Do some cleaning here\n",
        "    customer_name = tools.get_customer(self.path.lower())\n",
        "    if customer_name == \"Customer Regex Failed\":\n",
        "      vprint(\"Customer name resolution failed:%s\\n\"%(self.path) , end=\"\", flush=True)\n",
        "    self.df['CUSTOMER'] = customer_name\n",
        "    self.df[\"file_path\"] = self.path\n",
        "\n",
        "    specialty_columns = ['RXPRICE', 'DISPENSINGFEE', 'UNITPRICE']\n",
        "    for col in specialty_columns:\n",
        "      try:\n",
        "        self.df[col+'TYPE'] = self.df[col].apply(type)\n",
        "        if len(self.df.index[self.df[col+'TYPE'] == str].tolist()) > 0:\n",
        "          self.df = self.df.truncate(\n",
        "              after=list([a-1 for a in self.df.index[(self.df[col+'TYPE'] == str)].tolist()])[0]\n",
        "              ,axis=0\n",
        "              )\n",
        "          # self.df.drop(labels=[1+a for a in self.df.index[(self.df[col+'TYPE'] == str)].tolist()], axis=0, inplace=True)\n",
        "          # self.df.drop(labels=self.df.index[(self.df[col+'TYPE'] == str)].tolist(), axis=0, inplace=True)\n",
        "        # drop calculations after meaningful data if known to exist\n",
        "      except Exception as e:\n",
        "        #print(e)\n",
        "        continue\n",
        "\n",
        "  def simplify(self):    \n",
        "    if type(self.df) == type(None):\n",
        "      vprint(\"0\\t\\\"Invoice Failed to Resolve\\\"\\t\\\"%s\\\"\\t\\\"%s\\\"\"%(self.path, self.sheet), log=True, flush=True)\n",
        "      return None\n",
        "\n",
        "    vprint(\"simplifying - %d\\n\"%(self.file_num) , end=\"\",flush=True)\n",
        "\n",
        "    _columns = self.df.columns.tolist()\n",
        "    simplified = pd.DataFrame()\n",
        "\n",
        "    self.df = self.df.convert_dtypes()\n",
        "    column_types = {\n",
        "        \"api_request_timestamp\": (lambda df, c: df[c].astype(str)),\n",
        "        \"zipcode\": (lambda df, c: df[c].astype(str)),\n",
        "        \"cash_insurance\": (lambda df, c: df[c].astype(str)),\n",
        "        \"dispensed_ndc\": (lambda df, c: df[c].astype(str)),\n",
        "        \"rx_ndc_concat\" : (lambda df, c: df[c].astype(str)),\n",
        "        \"rx_number\": (lambda df, c: df[c].astype(str)),\n",
        "        \"dispensing_fee\": (lambda df, c: pd.to_numeric(df[c], errors=\"coerce\")),\n",
        "        \"rx_price\": (lambda df, c: pd.to_numeric(df[c], errors=\"coerce\")),\n",
        "        \"unit_price\": (lambda df,c: pd.to_numeric(df[c], errors=\"coerce\"))\n",
        "    }\n",
        "\n",
        "    for col, reg in tools.column_name_regex:\n",
        "      rexcomp = re.compile(reg)\n",
        "      try:\n",
        "        simplified[col] = self.df[list(filter(rexcomp.match,_columns))].bfill(axis=1).iloc[:,0]\n",
        "        try:\n",
        "          simplified[col] = column_types[col](simplified,col)\n",
        "        except Exception as e1:\n",
        "          continue\n",
        "      except Exception as e2:\n",
        "        continue\n",
        "    \n",
        "    simplified['api_request_timestamp'] = pd.to_datetime(simplified['api_request_timestamp'], \n",
        "                                          #format='%Y-%m-%d %H:%M:%S', \n",
        "                                          utc=True,\n",
        "                                          errors='coerce')\n",
        "    \n",
        "    self.df = simplified\n",
        "    # vprint(self.df.dtypes)\n",
        "    # "
      ],
      "metadata": {
        "id": "7zIaNpmAAbzy"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CandidateFile:\n",
        "  def __init__(self,filepath, iter):\n",
        "    self.path = filepath\n",
        "    self.workbook = None\n",
        "    self.sheet = None\n",
        "    self.page = None\n",
        "    self.file_num = iter\n",
        "    \n",
        "    self.has_valid = False\n",
        "    self.is_bad = False\n",
        "    self.__regex__ = re.compile(\"\\s*api.*\")\n",
        "  \n",
        "  def load(self,lock):\n",
        "    lock.acquire()\n",
        "    vprint(\"Loading - %d\\n\"%(self.file_num) , end=\"\",flush=True)\n",
        "    \n",
        "    try:\n",
        "      self.workbook = pd.ExcelFile(self.path)\n",
        "      self.has_valid = self.verify_sheets()\n",
        "      self.workbook.close()\n",
        "    except Exception as e:\n",
        "      self.is_bad = True\n",
        "      vprint(\"0\\t\\\"Workbook Failed to Load\\\"\\t\\\"%s\\\"\\t\\\"%s\\\"\"%(self.path, self.sheet), log=True, flush=True)\n",
        "    finally:\n",
        "      lock.release()\n",
        "  \n",
        "  def verify_sheets(self):\n",
        "    for sheet in self.workbook.sheet_names:\n",
        "      try:\n",
        "          self.page = pd.read_excel(self.path, sheet_name=sheet)\n",
        "          trimmed = list(filter(self.__regex__.match, list(self.page.columns.str.lower())))\n",
        "          if len(trimmed) == 2:\n",
        "            self.sheet = sheet\n",
        "            return True\n",
        "      except Exception as e:\n",
        "        continue\n",
        "    return False\n",
        "  \n",
        "  def transfer_knowledge(self):\n",
        "    vprint(\"Transfering knowledge - %d\\n\"%(self.file_num) , end=\"\",flush=True)\n",
        "    if not self.is_bad and self.has_valid:\n",
        "      if type(self.page) != type(None) and self.page.empty == False:\n",
        "        return Invoice(self.page, self.path, self.sheet, self.file_num)\n",
        "    return None\n",
        "\n"
      ],
      "metadata": {
        "id": "aei3e44R8xat"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DocumentProcess:\n",
        "  def __init__(self, file, n):\n",
        "    self.candy = CandidateFile(file, n)\n",
        "    self.file_num = n\n",
        "  \n",
        "  def _run(self):\n",
        "    invoice = None\n",
        "    self.candy.load(fsys_lock)\n",
        "    if self.candy.is_bad:\n",
        "      vprint(\"Rotten Egg - %d\\n\"%(self.file_num) ,end=\"\", flush=True)\n",
        "      return None\n",
        "    if self.candy.has_valid:\n",
        "      try:\n",
        "        invoice = self.candy.transfer_knowledge()\n",
        "        if invoice != None:\n",
        "          invoice.preprocess()\n",
        "          invoice.simplify()\n",
        "          vprint(\"done prepping - %d\\n\"%(self.file_num) , end=\"\",flush=True)\n",
        "          \n",
        "          if type(invoice.df) != type(None) and not invoice.df.empty:\n",
        "            sql_lock.acquire()\n",
        "            try:\n",
        "              vprint(\"trying to sqlize - %d\\n\"%(self.file_num) , end=\"\",flush=True)\n",
        "              \n",
        "\n",
        "              tools.sqlize(invoice.df)\n",
        "\n",
        "              vprint(\"Shared %d to DB\\n\"%(self.file_num), end=\"\",flush=True)\n",
        "              \n",
        "            except Exception as e:\n",
        "              vprint(\"0\\t\\\"DB Share Failed\\\"\\t\\\"%s\\\"\\t\\\"%s\\\"\"%(self.candy.path, self.candy.sheet), log=True, flush=True)\n",
        "              vprint(\"%s\\nCould not send %d to Database because %s\\n %s\\n\"%(invoice.df.dtypes,self.file_num, e, traceback.format_exc()), end=\"\",flush=True)\n",
        "              \n",
        "            finally:\n",
        "              sql_lock.release()\n",
        "          else:\n",
        "            vprint(\"0\\t\\\"Invoice Empty\\\"\\t\\\"%s\\\"\\t\\\"%s\\\"\"%(self.candy.path, self.candy.sheet), log=True, flush=True)\n",
        "            vprint(\"Invoice is empty for some reason\\n\", end=\"\",flush=True)\n",
        "            return None\n",
        "          vprint(\"File %d Complete\\n\"%(self.file_num) , end=\"\",flush=True)\n",
        "          vprint(\"1\\t\\\"Upload Complete\\\"\\t\\\"%s\\\"\\t\\\"%s\\\"\"%(self.candy.path, self.candy.sheet), log=True, flush=True)\n",
        "          \n",
        "\n",
        "      except Exception as e:\n",
        "        vprint(\"Sheet %d could not be processed because %s \\n %s\\n\"%(self.file_num,e, traceback.format_exc()) , end=\"\",flush=True)\n",
        "        \n",
        "    else:\n",
        "      vprint(\"0\\t\\\"No Sheets Found\\\"\\t\\\"%s\\\"\"%(self.candy.path), log=True, flush=True)\n",
        "      vprint(\"No Sheets Here - %d\\n\"%(self.file_num) , end=\"\",flush=True)\n",
        "      \n",
        "\n",
        "    "
      ],
      "metadata": {
        "id": "-0IM7o-CR0gZ"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_active_globals(input_series):\n",
        "  intersection = [i for i in input_series if i in globals()]\n",
        "  for var in intersection:\n",
        "    del globals()[var]"
      ],
      "metadata": {
        "id": "y3fY2KYNtYyF"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def f(obj):\n",
        "    obj._run()\n",
        "\n",
        "def RunIngestionPipe(verbose=True, log=True, log_file=\"log.txt\", random_batch_size=None, max_pooling=5, refresh=False, debug=False, debug_list=None):\n",
        "  global verbose_, log_file_\n",
        "  verbose_ = verbose\n",
        "\n",
        "  if refresh or ('tools' not in globals()):\n",
        "    global tools\n",
        "    tools = Armory()\n",
        "    print(\"Traversing File Structure\")\n",
        "    tools.load()\n",
        "    print(\"All Files Explored\")\n",
        "  if tools.validate():\n",
        "    vprint(\"Tool Box is ready\\n\" , end=\"\",flush=True)\n",
        "  else:\n",
        "    raise Exception(\"Run Backtrace - Tool Set failed to validate\")\n",
        "    \n",
        "  global log_file_\n",
        "  log_file_ = tools.config_[\"logging\"][\"parent_dir\"] + log_file\n",
        "  \n",
        "  global fsys_lock, sql_lock, print_lock\n",
        "  fsys_lock = Lock()\n",
        "  sql_lock = Lock()\n",
        "  print_lock = Lock()\n",
        "  \n",
        "  if debug:\n",
        "    candidate_series =  ([DocumentProcess(f, n) for n,f in enumerate(debug_list)])\n",
        "  else:\n",
        "    if random_batch_size is not None:\n",
        "      candidate_series = ([DocumentProcess(f, n) for n,f in enumerate(r.sample(tools.candidate_files,random_batch_size))])\n",
        "    else:\n",
        "      candidate_series = ([DocumentProcess(f, n) for n,f in enumerate(tools.candidate_files)])\n",
        "    \n",
        "  vprint(\"Candidates Identified\\n\" , end=\"\",flush=True)\n",
        "  vprint(\"Beginning Ingestion\\n\" , end=\"\",flush=True)\n",
        "  with Pool(max_pooling) as p:\n",
        "    p.map(f, candidate_series)"
      ],
      "metadata": {
        "id": "rS5CNCTMQjC0"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "problem_files = [\n",
        "     ]\n",
        "    \n",
        "RunIngestionPipe(\n",
        "    verbose=True\n",
        "    , max_pooling=15\n",
        "    , random_batch_size=25\n",
        "    # , debug=True\n",
        "    # , debug_list=problem_files\n",
        "    , refresh=True\n",
        "  )"
      ],
      "metadata": {
        "id": "ozxmxQsGoc8N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Code Reminders\n",
        "# global tools\n",
        "# tools = Armory()\n",
        "\n",
        "# tools.load()\n",
        "# if tools.validate():\n",
        "#   vprint(\"Ready to Go\")\n",
        "# else:\n",
        "#   vprint(\"Run Backtrace\")\n",
        "\n",
        "\n",
        "# from random import sample\n",
        "# global fsys_lock, sql_lock\n",
        "# fsys_lock = Lock()\n",
        "# sql_lock = Lock()\n",
        "\n",
        "# def f(obj):\n",
        "#   obj._run()\n",
        "\n",
        "# if __name__ == \"__main__\":\n",
        "#   vprint(\"Program Started\")\n",
        "#   #prime_candy = []\n",
        "#   #candidate_series =  ([DocumentProcess(f, n) for n,f in enumerate(prime_candy)])\n",
        "#   candidate_series = ([DocumentProcess(f, n) for n,f in enumerate(sample(tools.candidate_files,20))])\n",
        "#   vprint(\"Files Loaded\")\n",
        "#   with Pool(MAX_POOL) as p:\n",
        "#     p.map(f, candidate_series)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    # apireqid = re.compile(\"APIREQ.*ID.{0,2}\")\n",
        "    # apireqtime = re.compile(\"APIREQ.*TIME.*\")\n",
        "    # ins = re.compile(\"(INS)(URANCE)?(CASH)?\")\n",
        "    # zipc = re.compile(\".*ZIPCODE.*\")\n",
        "    # totprice = re.compile(\"TOT.*PRICE.*\")\n",
        "    # unitp = re.compile(\"UNITP.*\")\n",
        "    # shipc = re.compile(\"SHIP.*COST.*\")\n",
        "    # contrx = re.compile(\"CONT.*RX.*\")\n",
        "    # concat = re.compile(\"^CONCAT(ENATE)?(?:NDCRX|RXNDC)$\")\n",
        "\n",
        "    # simplified['api_request_id'] = self.df[list(filter(apireqid.match,_columns))].bfill(axis=1).iloc[:, 0]\n",
        "\n",
        "    # simplified['api_request_timestamp'] = self.df[list(filter(apireqtime.match, _columns))].bfill(axis=1).iloc[:, 0]\n",
        "    # simplified['api_request_timestamp'] = simplified['api_request_timestamp'].astype(str)\n",
        "    # simplified['api_request_timestamp'] = simplified['api_request_timestamp'].str.replace(r' ?UTC', '', regex=True)\n",
        "\n",
        "    # simplified['customer'] = self.df[['CUSTOMER']].bfill(axis=1).iloc[:, 0]\n",
        "\n",
        "    # try:\n",
        "    #   #simplified['bin'] = self.df[['BIN']].bfill(axis=1).iloc[:, 0]\n",
        "    #   simplified['copay_amount'] = self.df[['COPAYAMOUNT']].bfill(axis=1).iloc[:, 0]\n",
        "    #   simplified['zipcode'] = self.df[list(filter(zipc.match, _columns))].bfill(axis=1).iloc[:, 0]\n",
        "    #   simplified = simplified.astype({\"zipcode\":str},errors='raise')\n",
        "    #   simplified['dispensed_days_supply'] = self.df[[\"DISPENSEDDAYSSUPPLY\"]].bfill(axis=1).iloc[:, 0]\n",
        "    #   #simplified['dispensed_gpi'] = self.df[[\"DISPENSEDGPI\"]].bfill(axis=1).iloc[:, 0]\n",
        "    #   simplified['dispensing_fee'] = self.df[[\"DISPENSINGFEE\"]].bfill(axis=1).iloc[:, 0]\n",
        "    #   simplified['cash_insurance'] = self.df[list(filter(ins.match, _columns))].bfill(axis=1).iloc[:, 0]\n",
        "    #   simplified = simplified.astype({\"cash_insurance\":str},errors='raise')\n",
        "    #   simplified['rx_price'] = self.df[[\"RXPRICE\"]].bfill(axis=1).iloc[:,0]\n",
        "    #   simplified['otc_price'] = self.df[[\"OTCPRICE\"]].bfill(axis=1).iloc[:,0]\n",
        "    #   simplified['pharmacy'] = self.df[[\"PHARMACY\"]].bfill(axis=1).iloc[:,0]\n",
        "    #   simplified['quantity'] = self.df[[\"QUANTITY\"]].bfill(axis=1).iloc[:,0]\n",
        "    #   simplified['total_order_price'] = self.df[list(filter(totprice.match, _columns))].bfill(axis=1).iloc[:,0]\n",
        "    #   simplified['file_path'] = self.df['file_path']\n",
        "\n",
        "    #   simplified['dispensed_ndc'] = self.df[[\"DISPENSEDNDC\"]].bfill(axis=1).iloc[:, 0]\n",
        "    #   simplified = simplified.astype({\"dispensed_ndc\":str},errors='raise')\n",
        "    #   simplified['insurance_reimbursement'] = self.df[[\"INSURANCEREIMBURSEMENT\"]].bfill(axis=1).iloc[:,0]\n",
        "    #   simplified['unit_price'] = self.df[list(filter(unitp.match, _columns))].bfill(axis=1).iloc[:,0]\n",
        "    #   simplified['invoice_reason'] = self.df[[\"INVOICEREASON\"]].bfill(axis=1).iloc[:, 0]\n",
        "    #   simplified['shipping_cost'] = self.df[list(filter(shipc.match, _columns))].bfill(axis=1).iloc[:, 0]\n",
        "    #   simplified['contains_rx']= self.df[list(filter(contrx.match, _columns))].bfill(axis=1).iloc[:, 0]\n",
        "\n",
        "    #   simplified['rx_ndc_concat'] = self.df[list(filter(concat.match, _columns))].bfill(axis=1).iloc[:, 0]\n",
        "    #   simplified['rx_ndc_concat'] = simplified['rx_ndc_concat'].astype(str)\n",
        "\n",
        "    #   simplified['rx_number'] = self.df[[\"RXNUMBER\"]].bfill(axis=1).iloc[:,0]\n",
        "    #   simplified = simplified.astype({\"rx_number\":str},errors='raise')\n",
        "    # except:\n",
        "    #   None\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "QdJY13SYq_Jf"
      },
      "execution_count": 39,
      "outputs": []
    }
  ]
}